{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pipeline.DBHandler import DBHandler\n",
    "import cohere\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# semantic chunking imports\n",
    "from semantic_router.splitters import RollingWindowSplitter\n",
    "from semantic_router.encoders import CohereEncoder, OpenAIEncoder\n",
    "from semantic_router.utils.logger import logger\n",
    "\n",
    "logger.setLevel(\"WARNING\")  # reduce logs from splitter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "co = cohere.Client(api_key=os.getenv('COHERE_API_KEY'))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def semantic_chunking(encoder: Union[type(CohereEncoder), type(OpenAIEncoder())], directory_path: str, score_threshold: float = 0.4) -> list:\n",
    "\t\"\"\"\n",
    "    Use the semantic chunking to split the documents into semantic chunks\n",
    "    Args:\n",
    "        encoder: an embedding model to use for the semantic chunking\n",
    "        directory_path (str): path to the directory containing the documents\n",
    "        score_threshold (float): the score threshold for the encoder below which the split is made, between 0 and 1\n",
    "    Returns:\n",
    "        splits (list): list of the semantic chunks\n",
    "    \"\"\"\n",
    "\tencoder.score_threshold = score_threshold\n",
    "\tsplitter = RollingWindowSplitter(\n",
    "\t\t# Todo: adjust the parameters according to the dataset\n",
    "\t\tencoder=encoder,\n",
    "\t\tdynamic_threshold=False,\n",
    "\t\tmin_split_tokens=150,\n",
    "\t\tmax_split_tokens=400,\n",
    "\t\twindow_size=5,\n",
    "\t\tplot_splits=True,\n",
    "\t\tenable_statistics=True\n",
    "\t)\n",
    "\n",
    "\tsplits = []\n",
    "\tfor file_name in os.listdir(directory_path):\n",
    "\t\tprint(file_name)\n",
    "\t\tfile = open(f'{directory_path}/{file_name}', \"r\")\n",
    "\t\texample_faq = file.read()\n",
    "\t\tfile.close()\n",
    "\n",
    "\t\tcurrent_splits = splitter([example_faq])\n",
    "\t\tcomplete_current_splits = []\n",
    "\n",
    "\t\tfor i in range(len(current_splits)-1):\n",
    "\t\t\t# for more context, add 200 chars from the previous and next splits\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tsplit_to_add = {\n",
    "\t\t\t\t\t'text': ' '.join(current_splits[i].docs + current_splits[i + 1].docs[:200]),\n",
    "\t\t\t\t\t'origin_file': file_name\n",
    "\t\t\t\t}\n",
    "\t\t\t\tcomplete_current_splits.append(split_to_add)\n",
    "\t\t\telif i + 1 == len(current_splits):\n",
    "\t\t\t\tsplit_to_add = {\n",
    "\t\t\t\t\t'text': ' '.join(current_splits[i - 1].docs[-200:] + current_splits[i].docs),\n",
    "\t\t\t\t\t'origin_file': file_name\n",
    "\t\t\t\t}\n",
    "\t\t\t\tcomplete_current_splits.append(split_to_add)\n",
    "\t\t\telse:\n",
    "\t\t\t\tsplit_to_add = {\n",
    "\t\t\t\t\t'text': ' '.join(current_splits[i - 1].docs[-200:] + current_splits[i].docs + current_splits[i + 1].docs[:200]),\n",
    "\t\t\t\t\t'origin_file': file_name\n",
    "\t\t\t\t}\n",
    "\t\t\t\tcomplete_current_splits.append(split_to_add)\n",
    "\n",
    "\t\tsplits.extend(complete_current_splits)\n",
    "\treturn splits"
   ],
   "id": "ab8a05dae63ecd2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "doc_number = len(os.listdir('data/docs'))\n",
    "if doc_number >= 50:\n",
    "\tsplits = semantic_chunking(CohereEncoder(), 'data/docs', 0.4)\n",
    "else:\n",
    "\tprint(f'The number of documents is {doc_number}')"
   ],
   "id": "c87c4450b7213f38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T12:32:26.907059Z",
     "start_time": "2024-10-25T12:32:26.899350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cohere_embedding(text: str, model) -> list:\n",
    "\t\"\"\"\n",
    "\tUse the Google Embedding API to embed the text\n",
    "\tArgs:\n",
    "\t\ttext (str): the text to embed\n",
    "\t\tmodel (str): the name of the model to use for the embedding\n",
    "\tReturns:\n",
    "\t\tembedding (list): the embedding vector of the text\n",
    "\tRaises:\n",
    "\t\tException: if there is an error in embedding the text\n",
    "\t\"\"\"\n",
    "\ttry:\n",
    "\t\tembedding = list(model.encode(text))\n",
    "\t\tembedding = [float(num) for num in embedding]\n",
    "\texcept Exception as e:\n",
    "\t\traise Exception(f'Error in embedding the text: {e}')\n",
    "\n",
    "\treturn embedding\n",
    "\n",
    "def google_embedding(text: str, model: str = 'models/text-embedding-004') -> list:\n",
    "\t\"\"\"\n",
    "\tUse the Google Embedding API to embed the text\n",
    "\tArgs:\n",
    "\t\ttext (str): the text to embed\n",
    "\t\tmodel (str): the name of the model to use for the embedding, either 'models/text-embedding-004' or 'models/embedding-001'\n",
    "\tReturns:\n",
    "\t\tembedding (list): the embedding vector of the text\n",
    "\tRaises:\n",
    "\t\tException: if there is an error in embedding the text\n",
    "\t\"\"\"\n",
    "\ttry:\n",
    "\t\tembedding = genai.embed_content(model=model, content=text, task_type='retrieval_document')\n",
    "\texcept Exception as e:\n",
    "\t\traise Exception(f'Error in embedding the text: {e}')\n",
    "\t\t\n",
    "\treturn embedding['embedding']\n",
    "\n",
    "def create_chunks(splits, emb_type: str) -> list:\n",
    "\t\"\"\"\n",
    "\tCreate the chunks of the documents and embed them\n",
    "\tArgs:\n",
    "\t\tsplits (list): list of the splits of the documents\n",
    "\t\temb_type (str): the type of the embedding to use, either 'emb1', 'emb2', or 'emb3'\n",
    "\tReturns:\n",
    "\t\tchunks (list): list of the chunks (dicts) with their embeddings\n",
    "\t\"\"\"\n",
    "\tchunks = []\n",
    "\tif emb_type == 'emb1':\n",
    "\t\tmodel = 'models/text-embedding-004'\n",
    "\t\tfor split in splits:\n",
    "\t\t\tembedding = google_embedding(split['text'], model)\n",
    "\t\t\tchunk = {\n",
    "\t\t\t\t'text': split['text'],\n",
    "\t\t\t\t'embedding': embedding,\n",
    "\t\t\t\t'origin_file': split['origin_file']\n",
    "\t\t\t}\n",
    "\t\t\tchunks.append(chunk)\n",
    "\t\treturn chunks\n",
    "\t\n",
    "\tif emb_type == 'emb2':\n",
    "\t\tmodel = 'models/embedding-001'\n",
    "\t\tfor split in splits:\n",
    "\t\t\tembedding = google_embedding(split['text'], model)\n",
    "\t\t\tchunk = {\n",
    "\t\t\t\t'text': split['text'],\n",
    "\t\t\t\t'embedding': embedding,\n",
    "\t\t\t\t'origin_file': split['origin_file']\n",
    "\t\t\t}\n",
    "\t\t\tchunks.append(chunk)\n",
    "\t\treturn chunks\n",
    "\t\t\n",
    "\tif emb_type == 'emb3':\n",
    "\t\tmodel = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\t\tfor split in splits:\n",
    "\t\t\tembedding = list(cohere_embedding(split['text'], model))\n",
    "\t\t\tembedding = [float(num) for num in embedding]\n",
    "\t\t\tchunk = {\n",
    "\t\t\t\t'text': split['text'],\n",
    "\t\t\t\t'embedding': embedding,\n",
    "\t\t\t\t'origin_file': split['origin_file']\n",
    "\t\t\t}\n",
    "\t\t\tchunks.append(chunk)\t\n",
    "\t\treturn chunks\n"
   ],
   "id": "ddb0ae97844285ff",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T12:35:52.100692Z",
     "start_time": "2024-10-25T12:33:09.980230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for emb in ['emb1', 'emb2', 'emb3']:\n",
    "\tprint(f'Creating embeddings for {emb}')\n",
    "\thandler = DBHandler(org_id=f'maccabi_{emb}', user_id='evaluator')\n",
    "\tchunks = create_chunks(splits, emb)\n",
    "\tprint(f'Updating the database with the embeddings for {emb}')\n",
    "\thandler.update('embeddings', chunks)"
   ],
   "id": "5a722a45e15738c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for emb1\n",
      "Updating the database with the embeddings for emb1\n",
      "Creating embeddings for emb2\n",
      "Updating the database with the embeddings for emb2\n",
      "Creating embeddings for emb3\n",
      "Updating the database with the embeddings for emb3\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fc4f144e6b2c025f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
