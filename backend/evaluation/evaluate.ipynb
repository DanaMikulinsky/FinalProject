{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluating the chatbot",
   "id": "ba863e86a3c1f395"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The ground truth data was manually created by the team and is stored in an Excel file. The file contains the following information:\n",
    "1. Question\n",
    "2. Correct answer\n",
    "3. The ID's of the relevant chunks that should be returned by the retriever"
   ],
   "id": "6fc862bb1271f871"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-24T21:43:39.145730Z",
     "start_time": "2024-10-24T21:43:34.687973Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from retrying import retry\n",
    "transformers.logging.set_verbosity_error()\n",
    "from backend.evaluation.Evaluator import Evaluator\n",
    "from backend.pipeline.DBHandler import DBHandler"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/student/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T21:43:46.473044Z",
     "start_time": "2024-10-24T21:43:46.366792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@retry(stop_max_attempt_number=3, wait_fixed=60*1000)  # 3 attempts, 60 seconds between retries\n",
    "def get_ground_truth_data(gt_file=\"./FAQ.xlsx\"):\n",
    "    # Load ground truth data from a file\n",
    "    data = pd.read_excel(gt_file)\n",
    "    QA_list = []\n",
    "    for i, row in data.iterrows():\n",
    "        relevant_chunks_id = row[\"relevant_chunks_id\"].strip().split(\",\")\n",
    "        QA_list.append((row[\"question\"], row[\"answer\"], relevant_chunks_id))\n",
    "    return QA_list\n",
    "\n",
    "ground_truth_data = get_ground_truth_data()"
   ],
   "id": "f8bf99d8cc48403",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T21:43:47.333308Z",
     "start_time": "2024-10-24T21:43:47.328669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_chatbot(ground_truth_data, style, embedding_type, search_method, llm_name):\n",
    "    db_handler = DBHandler(org_id=f'maccabi_{embedding_type}', user_id='evaluator', search_method=search_method)\n",
    "    #db_handler = DBHandler(org_id=f'maccabi', user_id='evaluator', search_method=search_method)\n",
    "    evaluator = Evaluator(db_handler, style=style, llm_model_name=llm_name)\n",
    "    results = evaluator.evaluate(ground_truth_data)\n",
    "    results['style'] = style if style != '' else 'neutral'\n",
    "    results['embedding_type'] = embedding_type\n",
    "    results['search_method'] = search_method\n",
    "    results['llm_name'] = llm_name\n",
    "    return results"
   ],
   "id": "c115af7236ff8243",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Evaluating the chatbot's answers with the ground truth data. Various configurations are tested:\n",
    "1. 3 llms\n",
    "2. 3 embedding types\n",
    "3. 2 search methods\n",
    "4. 5 styles"
   ],
   "id": "9d05a73ad7cd58b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T21:44:04.209113Z",
     "start_time": "2024-10-24T21:43:56.289441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "styles = ['', 'kids', 'elderly', 'emoji', 'rhymes'] # empty string means no style\n",
    "embedding_types = ['emb1', # models/text-embedding-004\n",
    "                   'emb2', # models/embedding-001\n",
    "                   'emb3'] # from HW1: SentenceTransformer('all-MiniLM-L6-v2')\n",
    "llm_names = ['gemini-1.5-flash', \n",
    "             'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', \n",
    "             'mistralai/Mistral-7B-Instruct-v0.1']\n",
    "search_methods = ['approximate', 'exact'] \n",
    "\n",
    "full_results = pd.DataFrame(columns=['style', 'embedding_type', 'search_method', 'llm_name', 'question', 'true_answer', 'chatbot_answer', 'cosine_similarity', 'correctness_score', 'faithfulness_score', 'retriever_scores'])\n",
    "\n",
    "for style in styles:\n",
    "    str_style = style if style != '' else 'neutral'\n",
    "    print(f\"Style: {str_style}\", end=\" | \")\n",
    "    for embedding_type in embedding_types:\n",
    "        print(f\"embedding_type: {embedding_type}\", end=\" | \")\n",
    "        for llm_name in llm_names:\n",
    "            print(f\"llm_name: {llm_name}\", end=\" | \")\n",
    "            for search_method in search_methods:\n",
    "                print(f\"search_method: {search_method} | \", end=\" Status: \")\n",
    "                try:\n",
    "                    results = evaluate_chatbot(ground_truth_data, style, embedding_type, search_method, llm_name)\n",
    "                    full_results = pd.concat([full_results, results], ignore_index=True)\n",
    "                    full_results.to_csv(\"full_results.csv\", index=False) #re-save after each iteration to be safe :)\n",
    "                    print(\"Done\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Configuration failed\")\n",
    "                    print(f\"Error content: {e}\") \n",
    "                    continue"
   ],
   "id": "130a1b2518ca9041",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style: neutral | embedding_type: emb1 | llm_name: gemini-1.5-flash | search_method: approximate Status: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/finalProject/backend/pipeline/Chatbot.py:184: UserWarning: No relevant context found in the database\n",
      "  warnings.warn('No relevant context found in the database')\n",
      "/home/student/finalProject/backend/pipeline/Chatbot.py:184: UserWarning: No relevant context found in the database\n",
      "  warnings.warn('No relevant context found in the database')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration failed\n",
      "Error content: Error in embedding the text: Invalid input: 'content' argument must not be empty. Please provide a non-empty value.\n",
      "search_method: exact Status: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 22\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msearch_method: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msearch_method\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Status: \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 22\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_chatbot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mground_truth_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstyle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msearch_method\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mllm_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m     full_results \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([full_results, results], ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     24\u001B[0m     full_results\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull_results.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;66;03m#re-save after each iteration to be safe :)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[5], line 5\u001B[0m, in \u001B[0;36mevaluate_chatbot\u001B[0;34m(ground_truth_data, style, embedding_type, search_method, llm_name)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#db_handler = DBHandler(org_id=f'maccabi', user_id='evaluator', search_method=search_method)\u001B[39;00m\n\u001B[1;32m      4\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m Evaluator(db_handler, style\u001B[38;5;241m=\u001B[39mstyle, llm_model_name\u001B[38;5;241m=\u001B[39mllm_name)\n\u001B[0;32m----> 5\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mevaluator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mground_truth_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstyle\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m style \u001B[38;5;28;01mif\u001B[39;00m style \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneutral\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      7\u001B[0m results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding_type\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m embedding_type\n",
      "File \u001B[0;32m~/finalProject/backend/evaluation/Evaluator.py:24\u001B[0m, in \u001B[0;36mEvaluator.evaluate\u001B[0;34m(self, ground_truth_data)\u001B[0m\n\u001B[1;32m     21\u001B[0m results \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue_answer\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchatbot_answer\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcosine_similarity\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     22\u001B[0m                                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcorrectness_score\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfaithfulness_score\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mretriever_scores\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m question, true_answer, rel_chunks_ids \u001B[38;5;129;01min\u001B[39;00m ground_truth_data:\n\u001B[0;32m---> 24\u001B[0m     chatbot_answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchatbot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manswer_question\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompare_answers(question, true_answer, chatbot_answer, rel_chunks_ids)\n\u001B[1;32m     26\u001B[0m     result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m question\n",
      "File \u001B[0;32m~/finalProject/backend/pipeline/Chatbot.py:196\u001B[0m, in \u001B[0;36mChatbot.answer_question\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21manswer_question\u001B[39m(\u001B[38;5;28mself\u001B[39m, query: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    189\u001B[0m \u001B[38;5;250m\t\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;124;03m\tRun a user's question through the RAG pipeline and return the answer\u001B[39;00m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;124;03m\tArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;124;03m\t\tstr: The answer to the user's question\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;124;03m\t\"\"\"\u001B[39;00m\n\u001B[0;32m--> 196\u001B[0m \trephrased_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrephrase_question\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    197\u001B[0m \tcontext \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_relevant_context(rephrased_query)\n\u001B[1;32m    198\u001B[0m \trag_prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;124m\t\t\t\tYou are a health care provider\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms assistant.\u001B[39m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;124m\t\t\t\tAnswer the user\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms question based solely on the provided information.\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;124m\t\t\t\t\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstyle_instructions\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;124m\t\t\t\t\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n",
      "File \u001B[0;32m~/finalProject/backend/pipeline/Chatbot.py:136\u001B[0m, in \u001B[0;36mChatbot.rephrase_question\u001B[0;34m(self, query, history_length)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;66;03m# Only use the last 5 messages in the chat history to keep the context relevant\u001B[39;00m\n\u001B[1;32m    130\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;124m\t\tYour job is to rephrase the last user\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms question as a standalone question that can be understood without\u001B[39m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;124m\t\tthe context that is provided in the chat history.\u001B[39m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;124m\t\tIf the user\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms question is already standalone, just return it as it is.\u001B[39m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;124m\t\tChat history: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mchat_history[\u001B[38;5;241m-\u001B[39mhistory_length:]\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;124m\t\t\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m--> 136\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minteract\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/finalProject/backend/pipeline/Chatbot.py:107\u001B[0m, in \u001B[0;36mChatbot.google_interact\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m    105\u001B[0m llm \u001B[38;5;241m=\u001B[39m genai\u001B[38;5;241m.\u001B[39mGenerativeModel(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_model_name)\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 107\u001B[0m \tresponse \u001B[38;5;241m=\u001B[39m \u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m \t\u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39mstrip(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mstrip(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/anaconda/envs/finalProject/lib/python3.10/site-packages/google/generativeai/generative_models.py:331\u001B[0m, in \u001B[0;36mGenerativeModel.generate_content\u001B[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001B[0m\n\u001B[1;32m    329\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m generation_types\u001B[38;5;241m.\u001B[39mGenerateContentResponse\u001B[38;5;241m.\u001B[39mfrom_iterator(iterator)\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 331\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_content\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    332\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrequest_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    334\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m generation_types\u001B[38;5;241m.\u001B[39mGenerateContentResponse\u001B[38;5;241m.\u001B[39mfrom_response(response)\n\u001B[1;32m    336\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m google\u001B[38;5;241m.\u001B[39mapi_core\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mInvalidArgument \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/anaconda/envs/finalProject/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:827\u001B[0m, in \u001B[0;36mGenerativeServiceClient.generate_content\u001B[0;34m(self, request, model, contents, retry, timeout, metadata)\u001B[0m\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_universe_domain()\n\u001B[1;32m    826\u001B[0m \u001B[38;5;66;03m# Send the request.\u001B[39;00m\n\u001B[0;32m--> 827\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mrpc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    828\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    829\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretry\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    830\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    831\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    832\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m \u001B[38;5;66;03m# Done; return the response.\u001B[39;00m\n\u001B[1;32m    835\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m/anaconda/envs/finalProject/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001B[0m, in \u001B[0;36m_GapicCallable.__call__\u001B[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compression \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    129\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m compression\n\u001B[0;32m--> 131\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/anaconda/envs/finalProject/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:293\u001B[0m, in \u001B[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    289\u001B[0m target \u001B[38;5;241m=\u001B[39m functools\u001B[38;5;241m.\u001B[39mpartial(func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    290\u001B[0m sleep_generator \u001B[38;5;241m=\u001B[39m exponential_sleep_generator(\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_initial, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maximum, multiplier\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_multiplier\n\u001B[1;32m    292\u001B[0m )\n\u001B[0;32m--> 293\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mretry_target\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predicate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43msleep_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    298\u001B[0m \u001B[43m    \u001B[49m\u001B[43mon_error\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mon_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    299\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/anaconda/envs/finalProject/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:144\u001B[0m, in \u001B[0;36mretry_target\u001B[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sleep \u001B[38;5;129;01min\u001B[39;00m sleep_generator:\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 144\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mtarget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    145\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misawaitable(result):\n\u001B[1;32m    146\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001B[0;32m/anaconda/envs/finalProject/lib/python3.10/site-packages/google/api_core/timeout.py:120\u001B[0m, in \u001B[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;66;03m# Avoid setting negative timeout\u001B[39;00m\n\u001B[1;32m    118\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout \u001B[38;5;241m-\u001B[39m time_since_first_attempt)\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/anaconda/envs/finalProject/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:76\u001B[0m, in \u001B[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(callable_)\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21merror_remapped_callable\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 76\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcallable_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m grpc\u001B[38;5;241m.\u001B[39mRpcError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     78\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mfrom_grpc_error(exc) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "File \u001B[0;32m/anaconda/envs/finalProject/lib/python3.10/site-packages/grpc/_channel.py:1178\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m   1166\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m   1167\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1168\u001B[0m     request: Any,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1173\u001B[0m     compression: Optional[grpc\u001B[38;5;241m.\u001B[39mCompression] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1174\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m   1175\u001B[0m     (\n\u001B[1;32m   1176\u001B[0m         state,\n\u001B[1;32m   1177\u001B[0m         call,\n\u001B[0;32m-> 1178\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_blocking\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1179\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcredentials\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwait_for_ready\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompression\u001B[49m\n\u001B[1;32m   1180\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1181\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m/anaconda/envs/finalProject/lib/python3.10/site-packages/grpc/_channel.py:1162\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._blocking\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m   1145\u001B[0m state\u001B[38;5;241m.\u001B[39mtarget \u001B[38;5;241m=\u001B[39m _common\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_target)\n\u001B[1;32m   1146\u001B[0m call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_channel\u001B[38;5;241m.\u001B[39msegregated_call(\n\u001B[1;32m   1147\u001B[0m     cygrpc\u001B[38;5;241m.\u001B[39mPropagationConstants\u001B[38;5;241m.\u001B[39mGRPC_PROPAGATE_DEFAULTS,\n\u001B[1;32m   1148\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_method,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1160\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_registered_call_handle,\n\u001B[1;32m   1161\u001B[0m )\n\u001B[0;32m-> 1162\u001B[0m event \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1163\u001B[0m _handle_event(event, state, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_deserializer)\n\u001B[1;32m   1164\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m state, call\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001B[0m, in \u001B[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001B[0m, in \u001B[0;36mgrpc._cython.cygrpc._next_call_event\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001B[0m, in \u001B[0;36mgrpc._cython.cygrpc._next_call_event\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001B[0m, in \u001B[0;36mgrpc._cython.cygrpc._latent_event\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:62\u001B[0m, in \u001B[0;36mgrpc._cython.cygrpc._internal_latent_event\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:58\u001B[0m, in \u001B[0;36mgrpc._cython.cygrpc._interpret_event\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/tag.pyx.pxi:71\u001B[0m, in \u001B[0;36mgrpc._cython.cygrpc._BatchOperationTag.event\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/operation.pyx.pxi:138\u001B[0m, in \u001B[0;36mgrpc._cython.cygrpc.ReceiveInitialMetadataOperation.un_c\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:69\u001B[0m, in \u001B[0;36mgrpc._cython.cygrpc._metadata\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:70\u001B[0m, in \u001B[0;36mgenexpr\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:64\u001B[0m, in \u001B[0;36mgrpc._cython.cygrpc._metadatum\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m<string>:1\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(_cls, key, value)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "full_results.head(20)",
   "id": "bab0f9708fa48fc0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
