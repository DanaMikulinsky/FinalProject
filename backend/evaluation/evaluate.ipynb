{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-22T12:50:42.177889Z",
     "start_time": "2024-10-22T12:50:34.722934Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from backend.evaluation.evaluation import Evaluator\n",
    "from backend.pipeline.DBHandler import DBHandler"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/student/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:50:42.881903Z",
     "start_time": "2024-10-22T12:50:42.878122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_ground_truth_data(gt_file=\"./FAQ.xlsx\"):\n",
    "    # Load ground truth data from a file\n",
    "    data = pd.read_excel(gt_file)\n",
    "    QA_list = []\n",
    "    for i, row in data.iterrows():\n",
    "        if i == 3:\n",
    "            break\n",
    "        QA_list.append((row[\"question\"], row[\"answer\"]))\n",
    "    return QA_list"
   ],
   "id": "f8bf99d8cc48403",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:50:52.024360Z",
     "start_time": "2024-10-22T12:50:43.432784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ground_truth_data = get_ground_truth_data()\n",
    "db_handler = DBHandler(org_id='maccabi', user_id='evaluator')\n",
    "evaluator = Evaluator(db_handler)\n",
    "results = evaluator.evaluate(ground_truth_data)"
   ],
   "id": "130a1b2518ca9041",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m db_handler \u001B[38;5;241m=\u001B[39m DBHandler(org_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaccabi\u001B[39m\u001B[38;5;124m'\u001B[39m, user_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevaluator\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m Evaluator(db_handler)\n\u001B[0;32m----> 4\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mevaluator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mground_truth_data\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/finalProject/backend/evaluation/evaluation.py:27\u001B[0m, in \u001B[0;36mEvaluator.evaluate\u001B[0;34m(self, ground_truth_data)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m question, true_answer \u001B[38;5;129;01min\u001B[39;00m ground_truth_data:\n\u001B[1;32m     26\u001B[0m     chatbot_answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchatbot\u001B[38;5;241m.\u001B[39manswer_question(question)\n\u001B[0;32m---> 27\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompare_answers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrue_answer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchatbot_answer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m     result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m question\n\u001B[1;32m     29\u001B[0m     result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue_answer\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m true_answer\n",
      "File \u001B[0;32m~/finalProject/backend/evaluation/evaluation.py:143\u001B[0m, in \u001B[0;36mEvaluator.compare_answers\u001B[0;34m(self, question, true_answer, chatbot_answer)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompare_answers\u001B[39m(\u001B[38;5;28mself\u001B[39m, question, true_answer, chatbot_answer):\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m--> 143\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mretriever_scores\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_retriever_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    144\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcosine_similarity\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_cosine_similarity(true_answer, chatbot_answer),\n\u001B[1;32m    145\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcorrectness_score\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_correctness_score(true_answer, chatbot_answer),\n\u001B[1;32m    146\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfaithfulness_score\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_faithfulness_score(question, chatbot_answer)\n\u001B[1;32m    147\u001B[0m     }\n",
      "File \u001B[0;32m~/finalProject/backend/evaluation/evaluation.py:108\u001B[0m, in \u001B[0;36mEvaluator.get_retriever_score\u001B[0;34m(self, question, batch_size)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, answer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(answers):\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m answer\u001B[38;5;241m.\u001B[39mstrip()\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124myes\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 108\u001B[0m         relevant_chunks_id\u001B[38;5;241m.\u001B[39mappend(\u001B[43mbatch_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[1;32m    110\u001B[0m \u001B[38;5;66;03m# If we've processed 40 tasks, wait for a minute before continuing\u001B[39;00m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (i \u001B[38;5;241m+\u001B[39m batch_size) \u001B[38;5;241m%\u001B[39m (\u001B[38;5;241m40\u001B[39m \u001B[38;5;241m*\u001B[39m batch_size) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.set_option('display.max_colwidth',1)\n",
    "results.to_csv(\"results.csv\")\n",
    "results.head(10)"
   ],
   "id": "bab0f9708fa48fc0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
