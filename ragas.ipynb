{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31291f796d0515b4",
   "metadata": {},
   "source": [
    "# chat imports\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# embeddings and LLMS\n",
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.llms import Ollama, GPT4All\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings, OllamaEmbeddings\n",
    "\n",
    "# ragas imports\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import answer_correctness, answer_relevancy, faithfulness, context_recall, context_precision\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f8e4b7eefce595",
   "metadata": {},
   "source": [
    "def chrome_create_retriever(langchain_chunks, embeddings_model):\n",
    "    \"\"\"\n",
    "    Create a retriever with chromadb\n",
    "    Args:\n",
    "        langchain_chunks (list): list of langchain documents\n",
    "        embeddings_model (LangchainEmbeddings): the embeddings model\n",
    "    Returns:\n",
    "        retriever (VectorStoreRetriever): retriever object based on a local chromadb\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed chunks and create vectorstore retriever\n",
    "    db = Chroma.from_documents(langchain_chunks, embeddings_model)\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def create_rag_chain(tested_llm, tested_retriever):\n",
    "    \"\"\"\n",
    "    Create the RAG chain using the tested_llm and retriever objects\n",
    "    Args:\n",
    "        tested_llm (LangchainLLM): the llm to be tested\n",
    "        tested_retriever (VectorStoreRetriever): the retriever to be tested\n",
    "    Returns:\n",
    "        rag_chain (RunnableSequence): langchain pipeline object\n",
    "    \"\"\"\n",
    "    # Define prompt template\n",
    "    template =\"\"\"\n",
    "            You are an assistant for question-answering tasks. \n",
    "            Use the following pieces of retrieved context to answer the question. \n",
    "            If you don't know the answer, say that you don't know. \n",
    "            Keep the answer concise.\n",
    "            Question: {question} \n",
    "            Context: {context} \n",
    "            Answer:\n",
    "            \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    # Setup RAG pipeline\n",
    "    rag_chain = (\n",
    "        {\"context\": tested_retriever,  \"question\": RunnablePassthrough()} \n",
    "        | prompt \n",
    "        | tested_llm\n",
    "        | StrOutputParser() \n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "def define_data(rag_chain, retriever, questions, ground_truths):\n",
    "    \"\"\"\n",
    "    Define the questions, ground truths, contexts and answers for the evaluation and convert them to a dataset\n",
    "    Args:\n",
    "        rag_chain (RunnableSequence): rag_chain object\n",
    "        retriever (VectorStoreRetriever): retriever object\n",
    "    Returns:\n",
    "        dataset (Dataset): dataset object\n",
    "    \"\"\"\n",
    "    answers = []\n",
    "    contexts = []\n",
    "\n",
    "    # Inference\n",
    "    for query in questions:\n",
    "        answers.append(rag_chain.invoke(query))\n",
    "        contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "\n",
    "    # To dict\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"ground_truths\": ground_truths,\n",
    "        \"contexts\": contexts,\n",
    "        \"answer\": answers,\n",
    "    }\n",
    "\n",
    "    # Convert dict to dataset\n",
    "    dataset = Dataset.from_dict(data)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def evaluate_ragas(dataset, critic_llm, critic_embeddings, llm_model_name, embedding_model_name):\n",
    "    \"\"\"\n",
    "    Evaluate the RAG using the critic_llm, critic_embeddings and RAGAS metrics\n",
    "    Args:\n",
    "        dataset (Dataset): dataset object\n",
    "        critic_llm (LangchainLLM): the critic llm\n",
    "        critic_embeddings (LangchainEmbeddings): the critic embeddings\n",
    "        llm_model_name (str): the name of the llm model\n",
    "        embedding_model_name (str): the name of the embeddings model\n",
    "    Returns:\n",
    "        results_df (pd.DataFrame): dataframe of the results\n",
    "        results_mean (pd.Series): series of the mean results to be used for comparison\n",
    "    \"\"\"\n",
    "    results = evaluate(\n",
    "        dataset,\n",
    "        metrics=[\n",
    "            answer_correctness,\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "        ],\n",
    "        llm=critic_llm,\n",
    "        embeddings=critic_embeddings,\n",
    "    )\n",
    "    results_df = results.to_pandas()\n",
    "    models_names_series = pd.Series([llm_model_name, embedding_model_name], index=['LLM', 'Embeddings'])\n",
    "    results_mean = pd.concat([models_names_series, results_df.iloc[:, 5:].mean()])\n",
    "\n",
    "    return results_df, results_mean\n",
    "\n",
    "\n",
    "def ragas_evaluation(tested_llm, llm_model_name, tested_embeddings, embedding_model_name, questions, ground_truths,\n",
    "                     critic_llm, critic_embeddings, path_to_docs_directory='docs', score_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Combine all the functions to preform the RAG assessment\n",
    "    Args:\n",
    "        tested_llm (LangchainLLM): the llm to be tested\n",
    "        llm_model_name (str): the name of the llm model\n",
    "        embedding_model_name (str): the name of the embeddings model\n",
    "        tested_embeddings (LangchainEmbeddings): the embeddings model\n",
    "        critic_llm (LangchainLLM): the critic llm\n",
    "        critic_embeddings (LangchainEmbeddings): the critic embeddings\n",
    "        path_to_docs_direcory (str): path to the directory containing the documents\n",
    "        score_threshold (float): the score threshold for the encoder below which the split is made, between 0 and 1\n",
    "    Returns:\n",
    "        results_df (pd.DataFrame): dataframe of the results\n",
    "        results_mean (pd.Series): series of the mean results to be used for comparison\n",
    "    \"\"\"\n",
    "    splits = semantic_chunking(CohereEncoder(), path_to_docs_directory, score_threshold)\n",
    "    langchain_chunks = splits_to_langchain(splits)\n",
    "    retriever = chrome_create_retriever(langchain_chunks, tested_embeddings)\n",
    "    print('Created retriever')\n",
    "    rag_chain = create_rag_chain(tested_llm, retriever)\n",
    "    print('Created rag_chain')\n",
    "    dataset = define_data(rag_chain, retriever, questions, ground_truths)\n",
    "    print('Defined data')\n",
    "    # critic_llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
    "    # critic_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    results_df, results_mean = evaluate_ragas(dataset, critic_llm, critic_embeddings, llm_model_name, embedding_model_name)\n",
    "\n",
    "    return results_df, results_mean\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2477de5f",
   "metadata": {},
   "source": [
    "metrics_explanations = {\n",
    "    'answer_correctness': 'Measures how accurate and correct the provided answer is.',\n",
    "    'context_precision': 'Assesses the proportion of relevant context that is accurately included.',\n",
    "    'faithfulness': 'Evaluates how well the answer remains true to the provided information.',\n",
    "    'answer_relevancy': 'Determines how pertinent and relevant the answer is to the question asked.',\n",
    "    'context_recall': 'Measures the extent to which relevant context is covered in the answer.',\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56006651",
   "metadata": {},
   "source": [
    "questions = [\"When was the Thunderhawks Basketball Team founded?\", \n",
    "            \"What are some notable achievements of the Thunderhawks Basketball Team?\",\n",
    "            \"Who are the members of the Thunderhawks Basketball Team?\",\n",
    "            ]\n",
    "ground_truths = [[\"The Thunderhawks Basketball Team was established in 1998.\"],\n",
    "                [\"Threetime champions of the Midwest Basketball League, Winners of the prestigious Thunderhawks Invitational Tournament in 2015.\"],\n",
    "                [\"John Johnson, Sarah Smith, Marcus Martinez, Jessice Lopez and Michael Thompson.\"]\n",
    "                ]\n",
    "\n",
    "\n",
    "# questions = [\"When was the Thunderhawks Basketball Team founded?\",\n",
    "#             ]\n",
    "# ground_truths = [[\"The Thunderhawks Basketball Team was established in 1998.\"],\n",
    "#                 ]\n",
    "\n",
    "# questions = [\"מהו תהליך הפוטוסינתזה?\", \n",
    "#           \"מי היה ג'וליאס קיסר ומה הייתה תרומתו להיסטוריה?\",\n",
    "#           \"מי היה ויליאם שייקספיר ומה הייתה תרומתו לעולם הספרות?\",\n",
    "#           ]\n",
    "# ground_truths = [[\"פוטוסינתזה הוא תהליך שבו צמחים, אצות וחיידקים מסוימים ממירים אור שמש, פחמן דו-חמצני ומים לאנרגיה כימית בדמות גלוקוז, תוך שחרור חמצן.\"],\n",
    "#               [\"יוליוס קיסר היה מדינאי, מצביא וסופר רומאי שחי במאה הראשונה לפני הספירה.\"],\n",
    "#               [\"ויליאם שייקספיר היה מחזאי, משורר ושחקן אנגלי שנחשב לאחד מהגדולים ביותר בעולם הספרות. הוא חי בין השנים 1564-1616 וכתב עשרות מחזות, שירים וסונטות.\"]\n",
    "#               ]\n",
    "\n",
    "\n",
    "tested_llm = Ollama(model=\"llama2\")\n",
    "llm_model_name = \"LLaMA-2\"\n",
    "embedding_model_name = \"LLaMA-2\"\n",
    "tested_embeddings = OllamaEmbeddings(model=\"llama2\")\n",
    "critic_llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
    "critic_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "llama2_df, llama2_mean = ragas_evaluation(tested_llm, llm_model_name, tested_embeddings, embedding_model_name, questions, ground_truths, critic_llm, critic_embeddings, path_to_docs_directory='docs')\n",
    "\n",
    "\n",
    "tested_llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
    "llm_model_name = \"Gemini pro\"\n",
    "embedding_model_name = \"LLaMA-2\"\n",
    "tested_embeddings = OllamaEmbeddings(model=\"llama2\")\n",
    "# tested_embeddings = GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\", gpt4all_kwargs={'allow_download': 'True'})\n",
    "critic_llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
    "critic_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "gemini_df, gemini_mean = ragas_evaluation(tested_llm, llm_model_name, tested_embeddings, embedding_model_name, questions, ground_truths, critic_llm, critic_embeddings, path_to_docs_directory='docs')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d3b798",
   "metadata": {},
   "source": [
    "all_results_df = pd.concat([llama2_mean, gemini_mean], axis=1)\n",
    "all_results_df.T"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13b09bf1",
   "metadata": {},
   "source": [
    "llama2_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f6ba0cb",
   "metadata": {},
   "source": [
    "gemini_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fea6a",
   "metadata": {},
   "source": [
    "# Test on a Hebrew dataset\n",
    "tested_llm = Ollama(model=\"llama2\")\n",
    "llm_model_name = \"Gemini-Pro\"\n",
    "embedding_model_name = \"Gemini-Pro\"\n",
    "tested_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "critic_llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
    "critic_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "llama2_df, llama2_mean = ragas_evaluation(tested_llm, llm_model_name, tested_embeddings, embedding_model_name, critic_llm, critic_embeddings, path_to_docs_direcory='docs_hebrew', score_threshold=0.8)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917f4b3",
   "metadata": {},
   "source": [
    "# Create a retriever with MongoDB Atlas Vector Search\n",
    "db_constants = {'connection_string': 'mongodb+srv://saardavid:B92PKmRRELT6GOlJ@kal-chat.dprn3sl.mongodb.net/',\n",
    "                'db': 'chatbots',\n",
    "                'histories_collection': 'chat_histories',\n",
    "                'atlas_vector_search_index_name': 'vector_index'}\n",
    "# Creating vector search based on a collection\n",
    "vector_search = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "    db_constants['connection_string'],\n",
    "    db_constants['db'] + \".\" + '65df400432461f3d9dc38e8f_65ed84d657cd7b4874103e1a_vdb',\n",
    "    OllamaEmbeddings(model=\"llama2\"),\n",
    "    index_name=db_constants['atlas_vector_search_index_name'],\n",
    ")\n",
    "\n",
    "retriever = vector_search.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
