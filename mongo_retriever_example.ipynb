{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import math\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# MongoDB\n",
    "from pymongo import MongoClient\n",
    "from pymongo.operations import SearchIndexModel\n",
    "import mplcursors\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe0ea37c060a7b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T17:07:25.160921Z",
     "start_time": "2024-07-09T17:07:25.149216Z"
    }
   },
   "source": [
    "# CONNECTION_STRING = \"mongodb+srv://saar_david:saar@kal-media.ggmvds5.mongodb.net/\"\n",
    "CONNECTION_STRING = \"mongodb://localhost:27018/\"\n",
    "DB_NAME = \"images\"\n",
    "COLLECTION_NAME = \"press_office\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bdf7bc8451c9eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T17:07:44.470467Z",
     "start_time": "2024-07-09T17:07:29.109567Z"
    }
   },
   "source": [
    "def build_db(sql_query_filepath=\"kalos-media-tagging.sql\"):\n",
    "    # Read sql file\n",
    "    with open(sql_query_filepath, 'r', encoding=\"utf8\") as file:\n",
    "        sql_query = file.readlines()\n",
    "\n",
    "    df = pd.DataFrame(sql_query)\n",
    "    df.columns = ['sql_query']\n",
    "    df = df.iloc[4:]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Define the regex pattern to extract values between single quotes\n",
    "    pattern = r\"\\('([^']*)','([^']*)','([^']*)','([^']*)'\\)\"\n",
    "    df[['url', 'descriptionHebrew', 'descriptionEnglish', 'tags']] = df['sql_query'].str.extract(pattern)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Use regular expression to extract English words\n",
    "def extract_english_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.replace(\"tags\", \"\")\n",
    "    english_words = re.findall(r'\\b[a-zA-Z0-9.]+\\b', text)\n",
    "    english_string = ' '.join(english_words)\n",
    "    return english_string\n",
    "\n",
    "\n",
    "# Use regular expression to extract Hebrew words\n",
    "def extract_hebrew_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.replace(\"tags\", \"\")\n",
    "    hebrew_words = re.findall(r'[א-ת0-9.]+', text)\n",
    "    hebrew_string = ' '.join(hebrew_words)\n",
    "    return hebrew_string\n",
    "\n",
    "\n",
    "def english_embeddings(df, embedding_model=SentenceTransformer('all-mpnet-base-v2')):\n",
    "    df['english_tags'] = df['tags'].apply(extract_english_words)\n",
    "    df['english_words'] = df['descriptionEnglish'].apply(extract_english_words)\n",
    "    df['english_tokens'] = df['english_tags'].astype(str) + ' ' + df['english_words'].astype(str)\n",
    "\n",
    "    # Embed df tokens - done only once\n",
    "    corpus_english = df['english_tokens'].values\n",
    "    corpus_embeddings_en = embedding_model.encode(corpus_english, convert_to_tensor=True)\n",
    "    embeddings_list = [embedding.cpu().detach().numpy() for embedding in corpus_embeddings_en]\n",
    "    df['embeddings_en'] = embeddings_list\n",
    "\n",
    "\n",
    "def hebrew_embeddings(df, embedding_model=SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')):\n",
    "    df['hebrew_tags'] = df['tags'].apply(extract_hebrew_words)\n",
    "    df['hebrew_words'] = df['descriptionHebrew'].apply(extract_hebrew_words)\n",
    "    df['hebrew_tokens'] = df['hebrew_tags'].astype(str) + ' ' + df['hebrew_words'].astype(str)\n",
    "\n",
    "    # Embed df tokens - done only once\n",
    "    corpus_hebrew = df['hebrew_tokens'].values\n",
    "    corpus_embeddings_he = embedding_model.encode(corpus_hebrew, convert_to_tensor=True)\n",
    "    embeddings_list = [embedding.cpu().detach().numpy() for embedding in corpus_embeddings_he]\n",
    "    df['embeddings_he'] = embeddings_list\n",
    "\n",
    "\n",
    "def cos_sim(row, query_embeddings, language_code='en'):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between the image embedding and the query embedding.\n",
    "    Args:\n",
    "        row(pd.Series): Row of the DataFrame containing the image embedding.\n",
    "        query_embeddings(torch.Tensor): Query embedding tensor.\n",
    "        language_code(str): Language code ('en' for English, 'he' for Hebrew).\n",
    "    Returns:\n",
    "        float: Cosine similarity between the image and the query embeddings.\n",
    "    \"\"\"\n",
    "    img_embedding = row[f'embeddings_{language_code}']\n",
    "\n",
    "    if isinstance(img_embedding, str):\n",
    "        try:\n",
    "            img_embedding = json.loads(img_embedding)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to convert description embedding from string to list.\")\n",
    "            return 0\n",
    "\n",
    "    if np.all(img_embedding == 0):\n",
    "        return 0\n",
    "\n",
    "    img_sim = (np.dot(img_embedding, query_embeddings) /\n",
    "               (np.linalg.norm(img_embedding) * np.linalg.norm(query_embeddings)))\n",
    "\n",
    "    return img_sim\n",
    "\n",
    "\n",
    "def check_input(input_string):\n",
    "    \"\"\"\n",
    "    Check if the input string contains only English or Hebrew letters, signs, and numbers.\n",
    "    Args:\n",
    "        input_string(str): Input string to check.\n",
    "    Returns:\n",
    "        str: 'en' if the input string is in English, 'he' if the input string is in Hebrew, False otherwise.\n",
    "                \n",
    "    \"\"\"\n",
    "    # Define regex patterns to match only English and Hebrew letters, signs, numbers, and spaces\n",
    "    english_pattern = r'^[a-zA-Z0-9\\s.,!?@#$%^&*()-_+=]*$'\n",
    "    hebrew_pattern = r'^[א-ת0-9\\s.,!?@#$%^&*()-_+=]*$'\n",
    "\n",
    "    if re.match(english_pattern, input_string):\n",
    "        return 'en'\n",
    "    elif re.match(hebrew_pattern, input_string):\n",
    "        return 'he'\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def display_images_in_grid(urls):\n",
    "    \"\"\"\n",
    "    Display images in a grid layout.\n",
    "    Args:\n",
    "        urls(list): List of image URLs.\n",
    "    \"\"\"\n",
    "    num_images = len(urls)\n",
    "    grid_size = (math.ceil(num_images ** 0.5), math.ceil(math.sqrt(num_images)))\n",
    "    figsize = (grid_size[1] * 3, grid_size[0] * 3)  # Adjust figsize based on the grid size\n",
    "\n",
    "    fig, axes = plt.subplots(*grid_size, figsize=figsize)\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    for ax, url in zip(axes, urls):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image from {url}: {e}\")\n",
    "            ax.axis('off')  # Hide the axis if the image cannot be loaded\n",
    "\n",
    "    # Hide any remaining axes if there are more grid cells than images\n",
    "    for ax in axes[len(urls):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_images_in_grid2(urls, descriptions):\n",
    "    \"\"\"\n",
    "    Display images one after the other with their descriptions.\n",
    "    Args:\n",
    "        urls (list): List of image URLs.\n",
    "        descriptions (list): List of descriptions corresponding to each image URL.\n",
    "    \"\"\"\n",
    "    num_images = len(urls)\n",
    "    figsize = (6, num_images * 5)  # Adjust figsize based on the number of images\n",
    "\n",
    "    fig, axes = plt.subplots(num_images, 1, figsize=figsize)\n",
    "\n",
    "    # Ensure axes is always iterable\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, url, desc in zip(axes, urls, descriptions):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(desc, fontsize=12, pad=10)  # Set the description as the title for the current image plot\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image from {url}: {e}\")\n",
    "            ax.axis('off')  # Hide the axis if the image cannot be loaded\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def search(df, n=5):\n",
    "    query = input(\"Please write a sentence which describes the image you want to fetch from DB:\\n\")\n",
    "    language_code = check_input(query)\n",
    "    while not language_code:\n",
    "        language_code = check_input(query)\n",
    "        print(\n",
    "            \"Search sentence you have entered is invalid. Please note that it must include only letters of one language, signs, and numbers.\")\n",
    "        query = input(\n",
    "            \"Please enter a sentence in english which describes the image you want to fetch from DB:\\n\")\n",
    "\n",
    "    if language_code == 'en':\n",
    "        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        query_embeddings = embedding_model.encode(query, convert_to_tensor=True).cpu()\n",
    "    else:\n",
    "        embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        query_embeddings = embedding_model.encode(query, convert_to_tensor=True).cpu()\n",
    "\n",
    "    df['cos_sim'] = df.apply(lambda row: cos_sim(row, query_embeddings, language_code), axis=1)\n",
    "    df = df.sort_values(by='cos_sim', ascending=False)\n",
    "    display_images_in_grid(df['url'].values[:n].tolist())\n",
    "    return df\n",
    "\n",
    "\n",
    "def mongo_search(connection, db_name, collection_name, query, filters, n=5):\n",
    "    \"\"\"\n",
    "    Search for similar images in MongoDB.\n",
    "    Args:\n",
    "        connection(str): MongoDB connection string.\n",
    "        db_name(str): Name of the database.\n",
    "        collection_name(str): Name of the collection.\n",
    "        query(list): List of embedding values.\n",
    "        filters(dict): Dictionary of filters.\n",
    "        n(int): Number of images to display.\n",
    "    Returns:\n",
    "        urls(list): URLs of the most similar images.\n",
    "    \"\"\"\n",
    "    client = MongoClient(connection)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pipeline = [\n",
    "        {\n",
    "            '$vectorSearch': {\n",
    "                'exact': False,\n",
    "                \"filter\": filters,\n",
    "                'index': 'hebrew_search_index',\n",
    "                'path': 'embeddings_he',\n",
    "                'queryVector': query,\n",
    "                'numCandidates': 3000,\n",
    "                'limit': n\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            '$project': {\n",
    "                '_id': 0,\n",
    "                'url': 1,\n",
    "                'descriptionHebrew': 1,\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    results = collection.aggregate(pipeline)\n",
    "    urls = []\n",
    "    description = []\n",
    "    for result in results:\n",
    "        urls.append(result['url'])\n",
    "        description.append(result['descriptionHebrew'])\n",
    "    return (urls, description)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db112e81be14c4b",
   "metadata": {},
   "source": [
    "df = build_db(sql_query_filepath=\"kalos-media-tagging.sql\")\n",
    "df = df.drop(columns=['sql_query']).dropna()\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "hebrew_embeddings(df)\n",
    "english_embeddings(df)\n",
    "df.drop(columns=['hebrew_tags', 'hebrew_words', 'hebrew_tokens', 'english_tags', 'english_words', 'english_tokens'],\n",
    "        inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e207f8311b08134b",
   "metadata": {},
   "source": [
    "# convert embeddings to list and insert to MongoDB\n",
    "df['embeddings_he'] = df['embeddings_he'].apply(lambda x: x.tolist())\n",
    "df['embeddings_en'] = df['embeddings_en'].apply(lambda x: x.tolist())\n",
    "\n",
    "df_dict = df.to_dict('records')\n",
    "# collection.insert_many(df_dict)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477fe98e590e2a43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T17:11:51.071620Z",
     "start_time": "2024-07-09T17:11:46.155402Z"
    }
   },
   "source": [
    "embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "v = embedding_model.encode(\"בנימין נתניהו\", convert_to_tensor=True).tolist()\n",
    "\n",
    "filters = {\n",
    "        \"$and\": [\n",
    "          {\n",
    "            \"filter_attempt\": {  \"$eq\": 1  }\n",
    "          },\n",
    "        ]\n",
    "      }"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d2d1fe3f290efd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T17:11:52.896964Z",
     "start_time": "2024-07-09T17:11:51.073425Z"
    }
   },
   "source": [
    "urls, description = mongo_search(CONNECTION_STRING, DB_NAME, COLLECTION_NAME, v, filters, n=100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d100c4a",
   "metadata": {},
   "source": [
    "urls"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2b72fd56",
   "metadata": {},
   "source": [
    "description"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5da0fa14",
   "metadata": {},
   "source": [
    "\n",
    "def display_images_in_grid2(urls, descriptions):\n",
    "    \"\"\"\n",
    "    Display images one after the other with their descriptions.\n",
    "    Args:\n",
    "        urls (list): List of image URLs.\n",
    "        descriptions (list): List of descriptions corresponding to each image URL.\n",
    "    \"\"\"\n",
    "    num_images = len(urls)\n",
    "    figsize = (6, num_images * 5)  # Adjust figsize based on the number of images\n",
    "\n",
    "    fig, axes = plt.subplots(num_images, 1, figsize=figsize)\n",
    "\n",
    "    # Ensure axes is always iterable\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, url, desc in zip(axes, urls, descriptions):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            title = ax.set_title(desc, fontsize=12, pad=10, loc = 'right')  # Set the description as the title for the current image plot\n",
    "            title.set_path_effects([path_effects.withStroke(linewidth=3, foreground='white')])  # Add a white background to the text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image from {url}: {e}\")\n",
    "            ax.axis('off')  # Hide the axis if the image cannot be loaded\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "43e91b04",
   "metadata": {},
   "source": [
    "def search_and_plot(query, n=5):\n",
    "    embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    v = embedding_model.encode(query, convert_to_tensor=True).tolist()\n",
    "\n",
    "    filters = {\n",
    "            \"$and\": [\n",
    "            {\n",
    "                \"filter_attempt\": {  \"$eq\": 1  }\n",
    "            },\n",
    "            ]\n",
    "        }\n",
    "    urls, description = mongo_search(CONNECTION_STRING, DB_NAME, COLLECTION_NAME, v, filters, n=n)\n",
    "    for i in range(len(urls)):\n",
    "        print(f\"Description: {description[i]}\")\n",
    "        # plot the image\n",
    "        response = requests.get(urls[i])\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        plt.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e182ea6e",
   "metadata": {},
   "source": [
    "search_and_plot(\"אנשים מתפללים בכותל\", n=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "928bb8cb",
   "metadata": {},
   "source": [
    "search_and_plot(\"תפילה\", n=10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3730094b",
   "metadata": {},
   "source": [
    "search_and_plot(\"ביבי\", n=10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7cbd26c7",
   "metadata": {},
   "source": [
    "search_and_plot(\"ירושלים\", n=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b228037a",
   "metadata": {},
   "source": [
    "search_and_plot(\"בנימין נתניהו\", n=5)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e857b4",
   "metadata": {},
   "source": [
    "search_and_plot(\"בטקס האזכרה\", n=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d36dfe8a",
   "metadata": {},
   "source": [
    "search_and_plot(\"בטקס האזכרה\", n=5)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8f6b58b93e942894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T17:11:56.686295Z",
     "start_time": "2024-07-09T17:11:52.898441Z"
    }
   },
   "source": [
    "display_images_in_grid(urls)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b92aa2ff5d2a59b7",
   "metadata": {},
   "source": [
    "# United Hatzala attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57db94e5543fd6",
   "metadata": {},
   "source": [
    "CONNECTION_STRING = \"mongodb+srv://saar_david:saar@kal-media.ggmvds5.mongodb.net/\"\n",
    "DB_NAME = \"kal-media\"\n",
    "COLLECTION_NAME = \"united-hatsala\"\n",
    "\n",
    "client = MongoClient(CONNECTION_STRING)\n",
    "db = client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "new_collection = db['united_hatzala_embeddings']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cbf98691a5f81e",
   "metadata": {},
   "source": [
    "# Load embeddings models\n",
    "english_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "hebrew_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Fetch the first 50,000 documents from the original collection\n",
    "cursor = collection.find().limit(50000)\n",
    "documents = list(cursor)\n",
    "\n",
    "# Convert to DataFrame for easier processing\n",
    "df = pd.DataFrame(documents)\n",
    "\n",
    "# Extract English and Hebrew words\n",
    "df['english_words'] = df['descriptionEnglish'].apply(extract_english_words)\n",
    "df['hebrew_words'] = df['descriptionHebrew'].apply(extract_hebrew_words)\n",
    "\n",
    "# # Create tokens\n",
    "# df['english_tokens'] = df['tags'].apply(extract_english_words) + ' ' + df['english_words']\n",
    "# df['hebrew_tokens'] = df['tags'].apply(extract_hebrew_words) + ' ' + df['hebrew_words']\n",
    "\n",
    "# Generate English embeddings\n",
    "corpus_english = df['english_words'].values\n",
    "corpus_embeddings_en = english_model.encode(corpus_english, convert_to_tensor=True)\n",
    "df['embeddings_en'] = [embedding.cpu().detach().numpy() for embedding in corpus_embeddings_en]\n",
    "\n",
    "# Generate Hebrew embeddings\n",
    "corpus_hebrew = df['hebrew_words'].values\n",
    "corpus_embeddings_he = hebrew_model.encode(corpus_hebrew, convert_to_tensor=True)\n",
    "df['embeddings_he'] = [embedding.cpu().detach().numpy() for embedding in corpus_embeddings_he]\n",
    "\n",
    "# Insert documents with embeddings into the new collection\n",
    "new_documents = df.to_dict(orient='records')\n",
    "for doc in new_documents:\n",
    "    doc['embeddings_en'] = doc['embeddings_en'].tolist()\n",
    "    doc['embeddings_he'] = doc['embeddings_he'].tolist()\n",
    "    new_collection.insert_one(doc)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e41f5cb8e72f3",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
