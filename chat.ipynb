{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-20T08:55:54.025326Z",
     "start_time": "2024-10-20T08:55:53.537219Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "from pipeline.DBHandler import DBHandler\n",
    "\n",
    "# chat imports\n",
    "import google.generativeai as genai\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T22:21:22.856292Z",
     "start_time": "2024-10-14T22:21:22.843584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Chatbot:\n",
    "\t# Todo: Support multiple LLMs\n",
    "\tdef __init__(self, user_id: str, llm_model_name: str = 'gemini-1.5-flash',\n",
    "\t\t\t\t embedding_model_name: str = 'models/text-embedding-004'):\n",
    "\t\t\"\"\"\n",
    "\t\tInitializes the Chat object\n",
    "\t\tArgs:\n",
    "\t\t\tuser_id (str): The user id, used to connect to the correct collections in the database\n",
    "\t\t\tllm_model_name (str): The name of the model to use\n",
    "\t\t\tembedding_model_name (str): the name of the model to use for the embedding, either 'models/text-embedding-004' or 'models/embedding-001'\n",
    "\t\tRaises:\n",
    "\t\t\tValueError: If the db_handler is not an instance of DBHandler\n",
    "\t\t\tValueError: If the model name is not supported\n",
    "\t\t\tRuntimeError: If there was an error initializing the model\n",
    "\t\t\"\"\"\n",
    "\t\tself.db_handler = DBHandler(user_id, os.getenv('MONGODB_CONNECTION_STRING'))\n",
    "\n",
    "\t\t# Initialize the LLM\n",
    "\t\tpossible_models = []\n",
    "\t\tfor option in genai.list_models():\n",
    "\t\t\tif 'generateContent' in option.supported_generation_methods:\n",
    "\t\t\t\tpossible_models.append(option.name)\n",
    "\t\tif not llm_model_name and isinstance(llm_model_name, str) and f'models/{llm_model_name}' not in possible_models:\n",
    "\t\t\traise ValueError(f'Model name must be one of the following: {possible_models}')\n",
    "\t\ttry:\n",
    "\t\t\tself.llm = genai.GenerativeModel(llm_model_name)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise RuntimeError(f'Error initializing model: {e}')\n",
    "\n",
    "\t\t# Validate the embedding model name\n",
    "\t\tif not embedding_model_name or embedding_model_name not in ['models/text-embedding-004',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'models/embedding-001']:\n",
    "\t\t\traise ValueError('Invalid embedding model name')\n",
    "\t\tself.embedding_model_name = embedding_model_name\n",
    "\n",
    "\tdef interact(self, query: str) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tInteracts with the model\n",
    "\t\tArgs:\n",
    "\t\t\tquery (str): The query to interact with\n",
    "\t\tReturns:\n",
    "\t\t\tstr: The response from the model\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tresponse = self.llm.generate_content(query)\n",
    "\t\t\treturn response.text.strip('\\n').strip(' ')\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise RuntimeError(f'Error interacting with model: {e}')\n",
    "\n",
    "\tdef rephrase_question(self, query: str) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tRephrases the last user's question as a standalone question\n",
    "\t\tReturns:\n",
    "\t\t\tstr: The rephrased question\n",
    "\t\tRaises:\n",
    "\t\t\tValueError: If the query is not a non-empty string\n",
    "\t\t\"\"\"\n",
    "\t\tif not query or not isinstance(query, str):\n",
    "\t\t\traise ValueError('Query must be a non-empty string')\n",
    "\n",
    "\t\tchat_history = self.db_handler.get_history()\n",
    "\t\tchat_history.append(f'user: {query}')\n",
    "\n",
    "\t\t# Only use the last 5 messages in the chat history to keep the context relevant\n",
    "\t\tprompt = f\"\"\"\n",
    "\t\t\t   Your job is to rephrase the last user's question as a standalone question that can be understood without the context that is provided in the chat history.\n",
    "\t\t\t   If the user's question is already standalone, just return it as it is.\n",
    "\t\t\t   Chat history: {chat_history[-5:]}\n",
    "\t\t\t   \"\"\"\n",
    "\t\tresponse = self.interact(prompt)\n",
    "\t\treturn response\n",
    "\n",
    "\tdef google_embedding(self, text: str) -> list:\n",
    "\t\t\"\"\"\n",
    "\t\tEmbeds the text using the embedding model\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): the text to embed\n",
    "\t\tReturns:\n",
    "\t\t\tembedding (list): the embedding vector of the text\n",
    "\t\tRaises:\n",
    "\t\t\tException: if there is an error in embedding the text\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tembedding = genai.embed_content(model=self.embedding_model_name, content=text,\n",
    "\t\t\t\t\t\t\t\t\t\t\ttask_type='retrieval_document')\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise Exception(f'Error in embedding the text: {e}')\n",
    "\n",
    "\t\treturn embedding['embedding']\n",
    "\n",
    "\tdef answer_question(self, query: str) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tRun a user's question through the RAG pipeline and return the answer\n",
    "\t\tArgs:\n",
    "\t\t\tquery (str): The user's question\n",
    "\t\tReturns:\n",
    "\t\t\tstr: The answer to the user's question\n",
    "\t\t\"\"\"\n",
    "\t\trephrased_query = self.rephrase_question(query)\n",
    "\n",
    "\t\tquery_vector = self.google_embedding(rephrased_query)\n",
    "\t\trelevant_chunks = self.db_handler.search(query_vector)\n",
    "\t\tif relevant_chunks:\n",
    "\t\t\t# Todo: Improve the logic for selecting the context\n",
    "\t\t\tcontext = '\\n\\n\\n'.join([chunk['text'] for chunk in relevant_chunks if chunk['score'] > 0.3])\n",
    "\t\telse:\n",
    "\t\t\twarnings.warn('No relevant context found in the database')\n",
    "\t\t\tcontext = ''\n",
    "\n",
    "\t\trag_prompt = f\"\"\"\n",
    "\t\t\t\t   Your job is to answer the user's question based only on the context provided in the chat history.\n",
    "\t\t\t\t   If you are unable to answer the question based on the context provided, you can ask for more information.\n",
    "\t\t\t\t   User's question: {rephrased_query}\n",
    "\t\t\t\t   Context: {context}\n",
    "\t\t\t\t   \"\"\"\n",
    "\n",
    "\t\tresponse = self.interact(rag_prompt)\n",
    "\n",
    "\t\tmessages_to_append = [\n",
    "\t\t\t{\n",
    "\t\t\t\t'role': 'user',\n",
    "\t\t\t\t'content': rephrased_query\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t'role': 'bot',\n",
    "\t\t\t\t'content': response\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\n",
    "\t\tself.db_handler.update('history', messages_to_append)\n",
    "\n",
    "\t\treturn response\n",
    "\n"
   ],
   "id": "9c6ed799babadcfe",
   "execution_count": 105,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T22:21:24.646086Z",
     "start_time": "2024-10-14T22:21:23.638525Z"
    }
   },
   "cell_type": "code",
   "source": "chat = Chatbot(user_id='maccabi')",
   "id": "d9628e82edeb3bf2",
   "execution_count": 106,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T22:22:08.387896Z",
     "start_time": "2024-10-14T22:22:08.384779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('Can you explain the process of terminating a pregnancy?')\n",
    "print(response)"
   ],
   "id": "2f8556433244caaf",
   "execution_count": 109,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T22:23:16.383432Z",
     "start_time": "2024-10-14T22:23:16.380732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('What are the conditions that must be met so maccabi will cover the cost of the process?')\n",
    "print(response)"
   ],
   "id": "3f57d9f1c6278043",
   "execution_count": 111,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T22:24:10.677412Z",
     "start_time": "2024-10-14T22:24:08.640984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('What is the biggest planet in the solar system?')\n",
    "print(response)"
   ],
   "id": "7b6a3ee7a08554a9",
   "execution_count": 112,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "7bf99d1a705505c2",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
