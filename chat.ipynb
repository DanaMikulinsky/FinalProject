{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-20T10:36:16.286064Z",
     "start_time": "2024-10-20T10:36:15.935818Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "from pipeline.DBHandler import DBHandler\n",
    "\n",
    "# chat imports\n",
    "import google.generativeai as genai\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "from pipeline.Chatbot import Chatbot\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T10:36:16.471282Z",
     "start_time": "2024-10-20T10:36:16.467407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class Chatbot:\n",
    "# \t# Todo: Support multiple LLMs\n",
    "# \tdef __init__(self, user_id: str, llm_model_name: str = 'gemini-1.5-flash',\n",
    "# \t\t\t\t embedding_model_name: str = 'models/text-embedding-004'):\n",
    "# \t\t\"\"\"\n",
    "# \t\tInitializes the Chat object\n",
    "# \t\tArgs:\n",
    "# \t\t\tuser_id (str): The user id, used to connect to the correct collections in the database\n",
    "# \t\t\tllm_model_name (str): The name of the model to use\n",
    "# \t\t\tembedding_model_name (str): the name of the model to use for the embedding, either 'models/text-embedding-004' or 'models/embedding-001'\n",
    "# \t\tRaises:\n",
    "# \t\t\tValueError: If the db_handler is not an instance of DBHandler\n",
    "# \t\t\tValueError: If the model name is not supported\n",
    "# \t\t\tRuntimeError: If there was an error initializing the model\n",
    "# \t\t\"\"\"\n",
    "# \t\tself.db_handler = DBHandler(user_id, os.getenv('MONGODB_CONNECTION_STRING'))\n",
    "# \n",
    "# \t\t# Initialize the LLM\n",
    "# \t\tpossible_models = []\n",
    "# \t\tfor option in genai.list_models():\n",
    "# \t\t\tif 'generateContent' in option.supported_generation_methods:\n",
    "# \t\t\t\tpossible_models.append(option.name)\n",
    "# \t\tif not llm_model_name and isinstance(llm_model_name, str) and f'models/{llm_model_name}' not in possible_models:\n",
    "# \t\t\traise ValueError(f'Model name must be one of the following: {possible_models}')\n",
    "# \t\ttry:\n",
    "# \t\t\tself.llm = genai.GenerativeModel(llm_model_name)\n",
    "# \t\texcept Exception as e:\n",
    "# \t\t\traise RuntimeError(f'Error initializing model: {e}')\n",
    "# \n",
    "# \t\t# Validate the embedding model name\n",
    "# \t\tif not embedding_model_name or embedding_model_name not in ['models/text-embedding-004',\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'models/embedding-001']:\n",
    "# \t\t\traise ValueError('Invalid embedding model name')\n",
    "# \t\tself.embedding_model_name = embedding_model_name\n",
    "# \n",
    "# \tdef interact(self, query: str) -> str:\n",
    "# \t\t\"\"\"\n",
    "# \t\tInteracts with the model\n",
    "# \t\tArgs:\n",
    "# \t\t\tquery (str): The query to interact with\n",
    "# \t\tReturns:\n",
    "# \t\t\tstr: The response from the model\n",
    "# \t\t\"\"\"\n",
    "# \t\ttry:\n",
    "# \t\t\tresponse = self.llm.generate_content(query)\n",
    "# \t\t\treturn response.text.strip('\\n').strip(' ')\n",
    "# \t\texcept Exception as e:\n",
    "# \t\t\traise RuntimeError(f'Error interacting with model: {e}')\n",
    "# \n",
    "# \tdef rephrase_question(self, query: str) -> str:\n",
    "# \t\t\"\"\"\n",
    "# \t\tRephrases the last user's question as a standalone question\n",
    "# \t\tReturns:\n",
    "# \t\t\tstr: The rephrased question\n",
    "# \t\tRaises:\n",
    "# \t\t\tValueError: If the query is not a non-empty string\n",
    "# \t\t\"\"\"\n",
    "# \t\tif not query or not isinstance(query, str):\n",
    "# \t\t\traise ValueError('Query must be a non-empty string')\n",
    "# \n",
    "# \t\tchat_history = self.db_handler.get_history()\n",
    "# \t\tchat_history.append(f'user: {query}')\n",
    "# \n",
    "# \t\t# Only use the last 5 messages in the chat history to keep the context relevant\n",
    "# \t\tprompt = f\"\"\"\n",
    "# \t\t\t   Your job is to rephrase the last user's question as a standalone question that can be understood without the context that is provided in the chat history.\n",
    "# \t\t\t   If the user's question is already standalone, just return it as it is.\n",
    "# \t\t\t   Chat history: {chat_history[-5:]}\n",
    "# \t\t\t   \"\"\"\n",
    "# \t\tresponse = self.interact(prompt)\n",
    "# \t\treturn response\n",
    "# \n",
    "# \tdef google_embedding(self, text: str) -> list:\n",
    "# \t\t\"\"\"\n",
    "# \t\tEmbeds the text using the embedding model\n",
    "# \t\tArgs:\n",
    "# \t\t\ttext (str): the text to embed\n",
    "# \t\tReturns:\n",
    "# \t\t\tembedding (list): the embedding vector of the text\n",
    "# \t\tRaises:\n",
    "# \t\t\tException: if there is an error in embedding the text\n",
    "# \t\t\"\"\"\n",
    "# \t\ttry:\n",
    "# \t\t\tembedding = genai.embed_content(model=self.embedding_model_name, content=text,\n",
    "# \t\t\t\t\t\t\t\t\t\t\ttask_type='retrieval_document')\n",
    "# \t\texcept Exception as e:\n",
    "# \t\t\traise Exception(f'Error in embedding the text: {e}')\n",
    "# \n",
    "# \t\treturn embedding['embedding']\n",
    "# \n",
    "# \tdef answer_question(self, query: str) -> str:\n",
    "# \t\t\"\"\"\n",
    "# \t\tRun a user's question through the RAG pipeline and return the answer\n",
    "# \t\tArgs:\n",
    "# \t\t\tquery (str): The user's question\n",
    "# \t\tReturns:\n",
    "# \t\t\tstr: The answer to the user's question\n",
    "# \t\t\"\"\"\n",
    "# \t\trephrased_query = self.rephrase_question(query)\n",
    "# \n",
    "# \t\tquery_vector = self.google_embedding(rephrased_query)\n",
    "# \t\trelevant_chunks = self.db_handler.search(query_vector)\n",
    "# \t\tif relevant_chunks:\n",
    "# \t\t\t# Todo: Improve the logic for selecting the context\n",
    "# \t\t\tcontext = '\\n\\n\\n'.join([chunk['text'] for chunk in relevant_chunks if chunk['score'] > 0.3])\n",
    "# \t\telse:\n",
    "# \t\t\twarnings.warn('No relevant context found in the database')\n",
    "# \t\t\tcontext = ''\n",
    "# \n",
    "# \t\trag_prompt = f\"\"\"\n",
    "# \t\t\t\t   Your job is to answer the user's question based only on the provided context.\n",
    "# \t\t\t\t   If you are unable to answer the question based on the context provided, you can ask for more information.\n",
    "# \t\t\t\t   Even if you don't have the direct answer, provide the best response you can.\n",
    "# \t\t\t\t   User's question: {rephrased_query}\n",
    "# \t\t\t\t   Context: {context}\n",
    "# \t\t\t\t   \"\"\"\n",
    "# \n",
    "# \t\tresponse = self.interact(rag_prompt)\n",
    "# \n",
    "# \t\tmessages_to_append = [\n",
    "# \t\t\t{\n",
    "# \t\t\t\t'role': 'user',\n",
    "# \t\t\t\t'content': rephrased_query\n",
    "# \t\t\t},\n",
    "# \t\t\t{\n",
    "# \t\t\t\t'role': 'bot',\n",
    "# \t\t\t\t'content': response\n",
    "# \t\t\t}\n",
    "# \t\t]\n",
    "# \n",
    "# \t\tself.db_handler.update('history', messages_to_append)\n",
    "# \n",
    "# \t\treturn response\n",
    "\n"
   ],
   "id": "9c6ed799babadcfe",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T10:36:17.904681Z",
     "start_time": "2024-10-20T10:36:16.873480Z"
    }
   },
   "cell_type": "code",
   "source": "chat = Chatbot(user_id='maccabi')",
   "id": "d9628e82edeb3bf2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('Can you explain the process of terminating a pregnancy?')\n",
    "print(response)"
   ],
   "id": "2f8556433244caaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('What are the conditions that must be met so maccabi will cover the cost of the process?')\n",
    "print(response)"
   ],
   "id": "3f57d9f1c6278043",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('What is the biggest planet in the solar system?')\n",
    "print(response)"
   ],
   "id": "7b6a3ee7a08554a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test tzizo questions",
   "id": "955881431934adb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('What is the quarterly cost to visit a specialist?')\n",
    "print(response)"
   ],
   "id": "7bf99d1a705505c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('And how is it charged?')\n",
    "print(response)"
   ],
   "id": "4ded165462d35d9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('Can I come to a maccabi health institutions with my dog?')\n",
    "print(response)"
   ],
   "id": "e5101f950c4d016",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('Can I come to a health institutions with a service animal?')\n",
    "print(response)"
   ],
   "id": "c98e43f7c2c4c0a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('A Family doctor')\n",
    "print(response)"
   ],
   "id": "95441dfc88d1c76e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T10:36:25.332255Z",
     "start_time": "2024-10-20T10:36:22.484902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = chat.answer_question('Is there a treatment of eating disorders in Holon?')\n",
    "print(response)"
   ],
   "id": "8042a08031208284",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided information does not contain information on Holon.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T10:36:20.134147Z",
     "start_time": "2024-10-20T10:36:20.128700Z"
    }
   },
   "cell_type": "code",
   "source": "chat.rag_prompt",
   "id": "ab7de67d8b2a5d85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9a3268ba23dac205"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
